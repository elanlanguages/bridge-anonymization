# NVIDIA Triton Inference Server for Rehydra NER
#
# Usage:
#   1. First run setup.sh to copy model files
#   2. Start with: docker compose up
#   3. Test with: npm run test:triton
#
# Requirements:
#   - Docker with NVIDIA Container Toolkit
#   - NVIDIA GPU with CUDA support

services:
  triton:
    image: nvcr.io/nvidia/tritonserver:24.01-py3
    container_name: rehydra-triton
    ports:
      - "8000:8000"  # HTTP (for debugging/metrics)
      - "8001:8001"  # gRPC (primary inference endpoint)
      - "8002:8002"  # Prometheus metrics
    volumes:
      - ./models:/models
    command: >
      tritonserver
      --model-repository=/models
      --log-verbose=1
      --strict-model-config=false
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - CUDA_VISIBLE_DEVICES=0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped


