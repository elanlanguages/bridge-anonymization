// Triton Inference Server gRPC Protocol
// Based on NVIDIA Triton Inference Server API
// https://github.com/triton-inference-server/common/blob/main/protobuf/grpc_service.proto

syntax = "proto3";

package inference;

// Health and readiness check services
service GRPCInferenceService {
  // Check server liveness
  rpc ServerLive(ServerLiveRequest) returns (ServerLiveResponse) {}
  
  // Check server readiness
  rpc ServerReady(ServerReadyRequest) returns (ServerReadyResponse) {}
  
  // Check if a specific model is ready
  rpc ModelReady(ModelReadyRequest) returns (ModelReadyResponse) {}
  
  // Run inference on a model
  rpc ModelInfer(ModelInferRequest) returns (ModelInferResponse) {}
}

// Server liveness request
message ServerLiveRequest {}

// Server liveness response
message ServerLiveResponse {
  bool live = 1;
}

// Server readiness request
message ServerReadyRequest {}

// Server readiness response
message ServerReadyResponse {
  bool ready = 1;
}

// Model readiness request
message ModelReadyRequest {
  string name = 1;
  string version = 2;
}

// Model readiness response
message ModelReadyResponse {
  bool ready = 1;
}

// Inference request
message ModelInferRequest {
  // Model name
  string model_name = 1;
  
  // Model version (empty string means latest)
  string model_version = 2;
  
  // Optional request ID for tracking
  string id = 3;
  
  // Input tensors
  repeated InferInputTensor inputs = 4;
  
  // Requested output tensors
  repeated InferRequestedOutputTensor outputs = 5;
  
  // Raw input contents (binary data for each input tensor, in order)
  repeated bytes raw_input_contents = 6;
}

// Input tensor metadata
message InferInputTensor {
  // Tensor name (must match model input name)
  string name = 1;
  
  // Data type: INT64, FP32, etc.
  string datatype = 2;
  
  // Tensor shape
  repeated int64 shape = 3;
  
  // Tensor contents (for non-raw mode)
  InferTensorContents contents = 4;
}

// Requested output tensor
message InferRequestedOutputTensor {
  // Output tensor name
  string name = 1;
}

// Inference response
message ModelInferResponse {
  // Model name
  string model_name = 1;
  
  // Model version
  string model_version = 2;
  
  // Request ID (echoed from request)
  string id = 3;
  
  // Output tensors
  repeated InferOutputTensor outputs = 4;
  
  // Raw output contents (binary data for each output tensor, in order)
  repeated bytes raw_output_contents = 5;
}

// Output tensor metadata
message InferOutputTensor {
  // Tensor name
  string name = 1;
  
  // Data type
  string datatype = 2;
  
  // Tensor shape
  repeated int64 shape = 3;
  
  // Tensor contents (for non-raw mode)
  InferTensorContents contents = 4;
}

// Tensor contents for various data types
message InferTensorContents {
  // Boolean values
  repeated bool bool_contents = 1;
  
  // INT8, INT16, INT32 values
  repeated int32 int_contents = 2;
  
  // INT64 values
  repeated int64 int64_contents = 3;
  
  // UINT8, UINT16, UINT32 values
  repeated uint32 uint_contents = 4;
  
  // UINT64 values
  repeated uint64 uint64_contents = 5;
  
  // FP32 values
  repeated float fp32_contents = 6;
  
  // FP64 values
  repeated double fp64_contents = 7;
  
  // Bytes/string values
  repeated bytes bytes_contents = 8;
}


